\section{Word Embeddings}



\begin{frame}{Word Embeddings}
\MyLogo{Slides based on \href{https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/}{An introduction to word embeddings for text analysis} by Shane Lynn (2017)}


\begin{itemize}
\item Represent words as a vector of numbers
\item Every word has a unique word embedding (or “vector”), which is
  just a list of numbers for each word.  
\item Embeddings start being useful from 50-500 dimensions
  \\ LLMs typically are much larger
\item The embedding captures the  “meaning” of the word. 
\item  Similar words end up with similar embedding values
\item Context based word embeddings give a different vector depending
  on the context 

\end{itemize}



\end{frame}

\begin{frame}{Word Embeddings --- words as numbers}
\begin{itemize}
\item In the simplest case, each word is a number
\end{itemize}
\includegraphics[width=\linewidth]{img/one-hot-word-embedding-vectors-768x276.png}
\begin{itemize}\addtolength{\itemsep}{-0.5ex}
\item Too many dimensions
\item No shared information
\item Mainly zeros
\end{itemize}
\end{frame}
\begin{frame}{Word Embeddings --- words as vectors}
\begin{itemize}
\item We want fewer, more meaningful, dimensions
\end{itemize}
\includegraphics[width=\linewidth]{img/3-dimensional-word-embeddings-example-768x362.png}

\begin{taskb}
How would you add \eng{child}? or \eng{emperor}?
\end{taskb}

\end{frame}

\begin{frame}{Similar words should be close}

\begin{center}
\hspace*{-5mm}\includegraphics[width=1.1\linewidth]{img/word-vector-space-similar-words.jpg}
\end{center}

From  \url{http://suriyadeepan.github.io}.
\end{frame}

\begin{frame}{We can learn these from raw text}

  \begin{itemize}
  \item 
  In word embeddings, these  vectors are learned from corpora, where a word is represented originally by a vector made of other words it appears close to, with the dimensions then reduced.

\item 
  Transfer models, starting with BERT, instead used neural nets and attention, predicting the context 
  
\item 
In LLMs, models are constructed to predict the context words from a
centre word, or the centre word from a set of context words.
\item 
By training on large amounts of text, embeddings that model human
intuitions can be built.
\end{itemize}
\end{frame}



\begin{frame}{Meaning and distribution}
  \begin{itemize}
  \item  “Die Bedeutung eines Wortes liegt in seinem Gebrauch.” \\
    [The meaning of a word is its use in the language]
    \\ ~ \hfill — Ludwig Wittgenstein
  \item  “You shall know a word by the company it keeps!”
    \\ ~ — J. R. Firth (1957)
  \item Distributional hypothesis: difference of meaning correlates
with difference of distribution \hfill (Zellig Harris 1954)
\item “What people know when they say that they know a word is
not how to recite its dictionary definition – they know how to
use it [\ldots] in everyday discourse.”\hfill  (Miller 1986)
  \end{itemize}
\end{frame}

\begin{frame}{What is the meaning of “bardiwac”?}
  \begin{itemize}[<+->]
  \item \eng{He handed her her glass of \ul{bardiwac}.}
  \item \eng{Beef dishes are made to complement the \ul{bardiwacs}.}
  \item \eng{Nigel staggered to his feet, face flushed from too much \ul{bardiwac}.}
  \item \eng{Becmal, one of the lesser-known \ul{bardiwac} grapes, responds well to Australia’s sunshine.}
  \item \eng{I dined off bread and cheese and this excellent \ul{bardiwac}.}
  \item \eng{The drinks were delicious: blood-red \ul{bardiwac} as well as light, sweet Rhenish.}
  \item[$\Rightarrow$] \lex{bardiwac} is a heavy red alcoholic beverage made from grapes
  \end{itemize}

So  \lex{bardiwac} probably appears in the same contexts as \lex{red wine}, \ldots
  
\end{frame}



\begin{frame}{Semantic relations are also learned}


\includegraphics[width=\linewidth]{img/vocabulary-linear-relationships.jpg}

We can do arithmetic on the vectors 
\smallskip

\begin{tabular}{rcl}
  $\vec{king}$ + $\vec{woman}$ $-$ $\vec{man}$  & $\approx$ & $\vec{queen}$ \\[2ex]
  $\vec{Paris}$ $-$ $\vec{France}$ + $\vec{Germany}$  & $\approx$ & $\vec{Berlin}$
\end{tabular}



\end{frame}

\begin{frame}{Corpora contain stereotypes, ML learns them!}

\begin{itemize}
\item We can test if things are closer to $\vec{he}$ or $\vec{she}$
  \\[2ex]
  \begin{tabular}{lcr}
    $\vec{nurse}.\vec{she}$ & = & 0.38 \\
    $\vec{nurse}.\vec{he}$ & = & $-0.12$ \\
    $\vec{programmer}.\vec{she}$ & = & 0.07 \\
    $\vec{programmer}.\vec{he}$ & = & 0.28 \\
  \end{tabular}
\end{itemize}

\begin{itemize}
\item This is an accurate description of the state of the world described in the corpus
\item But may not be what we want to use as a basis for reasoning, \ldots
\end{itemize}

\citet{10.5555/3157382.3157584}

\end{frame}

\begin{frame}{Some words are gendered, some are not, \ldots}

\noindent\includegraphics[width=\linewidth]{img/bias.png}

\begin{itemize}
\item Also complicated interactions with adjectives, race and more
\item It is close to impossible to remove this bias from the model
\end{itemize}
\end{frame}
