\section{Some more general issues}








\begin{frame}{If ChatGPT Can Do It, It’s Not Worth Doing}

\href{https://www.insidehighered.com/opinion/blogs/just-visiting/2023/09/20/chatgpt-shows-way-toward-our-own-humanity}{If ChatGPT Can Do It, It’s Not Worth Doing}, 
\href{https://www.insidehighered.com/}{Inside Higher Ed} by  John Warner,  2023

\begin{itemize}
\item If a LLM can do a writing task similar to or better than humans
  then it is not worth doing
  \begin{itemize}
  \item College essays are largely soulless pro forma exercises
  \item Business consultants:
    \begin{quote}
      … creative tasks (‘Propose at least 10 ideas for a new shoe targeting an underserved market or sport.’), analytical tasks (‘Segment the footwear industry market based on users.’), writing and marketing tasks (‘Draft a press release marketing copy for your product.’) and persuasiveness tasks (‘Pen an inspirational memo to employees detailing why your product would outshine competitors.’).
    \end{quote}
    
  \item Producing feedback on student essays to a strict rubric
    (marking standard)
    \begin{itemize}
    \item The regimentation of writing is not necessarily improving it
    \end{itemize}
  \end{itemize}
\item The argument is not very well made (FCB), but I think the point is almost certainly true
\end{itemize}


\end{frame}

\begin{frame}{Drowning in AI-produced Nonsense}

\begin{itemize}
\item Untruths produced by ChatGPT found in WebSearch
\item BING served them up as facts
  \\ {\small \href{https://www.wired.com/story/fast-forward-chatbot-hallucinations-are-poisoning-web-search/}{Chatbot Hallucinations Are Poisoning Web Search} Wired, Will Knight, Oct 5, 2023 (accessed 2023-10-06)}
\item Search for papers on PubPeer found over 50 with the phrase \eng{Regenerate response} and 9 with \eng{As an AI language model, I …}
\item This also points to issues with peer review
  \\{\small 
\href{https://retractionwatch.com/2023/10/06/signs-of-undeclared-chatgpt-use-in-papers-mounting/}{Signs of undeclared ChatGPT use in papers mounting} \textit{Retraction Watch} October 6, 2023 Frederik Joelving (accessed 2023-10-03)}

\newpage 


\item Google demo shows AI summarising emails and then replying to them.
  \begin{quote}
    This seems to be the future A.I. promises. Endless content generated by robots, enjoyed by no one, clogging up everything, and wasting everyone’s time.
  \end{quote}
  {\small
    \href{https://newrepublic.com/article/177197/year-ai-came-culture}{The Year That A.I. Came for Culture}
\textit{New Republic} (2023-12) Lincoln Michel }


\end{itemize}


\end{frame}

\begin{frame}{Model Collapse}
\begin{itemize}%\addtolength{\itemsep}{-1ex}
\item Models trained on data generated by previous generations of
  models begin to lose information about the tails of the original
  data distribution;  eventually converge to a single point estimate
  with little variance
\item Two sources of error: statistical
  approximation error due to finite sampling, and functional
  approximation error due to imperfect models
  \begin{itemize}
  \item  Probable events are over-estimated 
  \item Improbable events are under-estimated 
  \end{itemize}
\item The generated data begins to contain improbable sequences and
  loses information about the tails of the original distribution.
\item It is essential to identify human data (but currently impossible)
  \\ 33-46\% of crowd workers used LLMs when completing their tasks
  %https://arxiv.org/abs/2306.07899
\end{itemize}
\citet{shumailov2023curse} ``The Curse of Recursion: Training on Generated Data Makes Models Forget''

\end{frame}

\begin{frame}{Humans in the loop}

\begin{itemize}
\item Microsoft travel uploaded several AI generated articles,
  including an Ottawa guide recommending that tourists dine
  at the Ottawa Food Bank ("go on an empty stomach")
\item Microsoft said this was \txx{human error}: It was a supervised AI, overseen by a human who should have caught the error.
\item But --- humans can't maintain vigilance watching for rare occurrences.
\item TSA consistently fail to spot the bombs and guns that red teams
   smuggle past their checkpoints
 \item This is called \txx{automation blindness} or \txx{automation inattention}
   \begin{itemize}
   \item Either the system is so poor it is not worth doing
   \item Or it is so good people just click OK every time
   \end{itemize}
 \end{itemize}
\href{https://pluralistic.net/2023/08/23/automation-blindness/\#humans-in-the-loop}{Supervised AI isn't} Cory Doctorow (Aug, 2023)

\end{frame}

\begin{frame}{The real AI fight}

\begin{itemize}
\item There is a large public struggle between
  \begin{itemize}
  \item Doomers --- who think AI will destroy humanity
  \item Accelerationists -- who think AI will save humanity
  \end{itemize}
\item But LLM are not AGI (Artificial General Intelligence)
  \\ they are Stochastic Parrots just repeating or assembling phrases based on probabilities and statistical patterns learned from vast datasets of text, without real understanding or awareness \citep{10.1145/3442188.3445922}
\item The AI debate distracts us from the main issues of
  \begin{itemize}
  \item \href{https://en.wikipedia.org/wiki/Algorithmic_bias}{algorithmic bias}
  \item \href{https://en.wikipedia.org/wiki/Ghost_work}{ghost labor}
  \item erosion of the rights of artists
  \end{itemize}
\end{itemize}
\href{https://pluralistic.net/2023/11/27/10-types-of-people/}{The real AI fight} Cory Doctorow (Nov, 2023)

\end{frame}

\begin{frame}{Large Language Models propagate race-based medicine}

\begin{itemize}
\item Assessed four large language models with eight different questions that were
interrogated five times each with a total of forty responses per a model
\item All models had examples
of perpetuating race-based medicine
\item Models were not always consistent in
their responses 
\item  LLMs are being proposed for use in the healthcare setting, with
some models already connecting to electronic health record systems. 
\item These LLMs could potentially cause harm by perpetuating debunked,
racist concepts. 
\end{itemize}
\href{https://doi.org/10.1101/2023.07.03.23292192}{Beyond the hype: large language models propagate race-based medicine }
\end{frame}

\begin{frame}{AI Hype in my field}


\begin{itemize}
\item The author surveys ten papers, one of which shows that dictionary entries can be made that are largely correct for medium to high frequency words of English \citep{10.1093/ijl/ecad021}
  \begin{itemize}
  \item These would have to be corrected, with no indication of where the errors were
  \item The LLM was trained on data that included dictionaries with entries for these words
  \end{itemize}
\item Results for low-frequency or new uses were not investigated
\item Results for other languages were much worse
\item The author concludes \eng{The conclusion is that a new age, that
    of the successful application of generative AI in lexicography,
    has dawned}
\item It's rubbish
\end{itemize}


\end{frame}

\begin{frame}{Some technical issues}
\MyLogo{\href{https://www.explainxkcd.com/wiki/index.php/Robert');_DROP_TABLE_Students;--}{Robert'); DROP TABLE Students;-- ``Little Bobby Tables''}}
\begin{itemize}
\item LLMs are non-deterministic, so you can't guarantee the same results each time (or even the same style/format).   This is made worse in practice by the fact that companies routinely change online models without notice.
\item Because instructions and input are the same: \txx{text}.   It is very hard to guarantee there are no bad instructions in the input, \ldots
\end{itemize}
\end{frame}
\begin{frame}{Little Bobby Ignore All Instructions}
  \begin{center}
    \includegraphics[width=0.6\textwidth]{img/little-bobby-ignore.jpg}
  \end{center}

  Based on \href{https://xkcd.com/327/}{Little Bobby Tables}
\end{frame}

\begin{frame}{Some more interesting papers}


\href{https://lwn.net/Articles/945504/}{AI from a legal perspective} Linux Weekly News by Jake Edge September 26, 2023


\href{https://annals-csis.org/proceedings/2023/pliks/3725.pdf}{An Evaluation of a Zero-Shot Approach to
Aspect-Based Sentiment Classification in Historic
German Stock Market Reports} (2023) Janos Borst, Lino Wehrheim, Andreas Niekler, Manuel Burghardt
Preprints of Communication Papers of the of the 18th Conference on Computer
Science and Intelligence Systems pp. 51–60

\href{https://blog.mathieuacher.com/GPTsChessEloRatingLegalMoves/}{Debunking the Chessboard: Confronting GPTs Against Chess Engines to Estimate Elo Ratings and Assess Legal Move Abilities} 
Mathieu Acher Blog, September 30, 2023, accessed 2023-10-18

\href{https://bdtechtalks.com/2023/05/17/llm-emergent-abilities-mirage/}{Are the emergent abilities of LLMs like GPT-4 a mirage?} TechTalks
By Ben Dickson -May 17, 2023, accessed 2023-10-19

\href{https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand}{God Help Us, Let's Try To Understand AI Monosemanticity} Scott Alexander (2023)

\href{https://www.technologyreview.com/2023/12/05/1084393/make-no-mistake-ai-is-owned-by-big-tech/amp/}{Make no mistake—AI is owned by Big Tech}  Amba Kak, Sarah Myers West, and Meredith Whittaker (2023-12-05) \textit{MIT Technology Review}

If we’re not careful, Microsoft, Amazon, and other large companies will leverage their position to set the policy agenda for AI, as they have in many other sectors.
\end{frame}

% %%
% %% Good source for a tutorial:
% \end{frame}

%\begin{frame}{Acknowledmegments}

% \begin{itemize}
% \item Some prompt examples taken from
%   \href{https://www.w3schools.com/gen_ai/chatgpt-3-5/index.php}{ChatGPT-3.5
%     Tutorial} from W3 Schools
% \item \url{https://rollbar.com/blog/how-to-debug-code-using-chatgpt/}
% \end{itemize}
% %% 

%\end{frame}

\begin{frame}{LLMs are impressive, BUT}

\begin{itemize}
\item  Just because that text seems coherent doesn’t mean the model behind it has
understood anything or is trustworthy
\item  Just because that answer was correct doesn’t mean the next one will be
\item  When a computer seems to “speak our language”, we’re the ones
doing the work of interpretation
\item Mitigating the risks of language technology requires understanding what is actually going on
  \begin{itemize}
  \item We need make sure using a LLM is giving us what we really need for a task
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{LLMs are useful for many tasks}

\begin{itemize}
\item  Formatting
\item  Coding (so long as you can check it)
\item  Documenting code or writing test suites  (so long as you can check it)
  
\item Transforming from one format to another
  \begin{itemize}
  \item We need make sure using a LLM is giving us what we really need for a task
  \end{itemize}
\end{itemize}


\end{frame}

